{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d599732",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfcfa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60b7700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen/Desktop/ai2-gemme3n-kaggle-hackathon/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-19 15:29:52 [__init__.py:244] Automatically detected platform cuda.\n",
      "GPU detected: NVIDIA GeForce RTX 3060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjknguyen3010\u001b[0m (\u001b[33mjknguyen3010-university-of-buffalo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "import os\n",
    "os.environ[\"TORCH_LOGS\"] = \"recompiles\"\n",
    "os.environ['TORCHDYNAMO_CACHE_SIZE_LIMIT'] = '999999999'\n",
    "\n",
    "import torch\n",
    "import torch._dynamo \n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from pprint import pprint\n",
    "\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# FOR INFERENCES WITH VLLM\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# FOR LOGGING\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 7473\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")['train']\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a664626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\n'\n",
      "           'Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and '\n",
      "           'May.\\n'\n",
      "           '#### 72',\n",
      " 'question': 'Natalia sold clips to 48 of her friends in April, and then she '\n",
      "             'sold half as many clips in May. How many clips did Natalia sell '\n",
      "             'altogether in April and May?'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb6443",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "313a8899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7d3c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_answer(text):\n",
    "    # The issue is that in your test, 'text' is a tuple, not a string.\n",
    "    # So 'if delim in text' is False, because 'delim' is not in the tuple.\n",
    "    # You should pass a string, not a tuple, to this function.\n",
    "    delim = \"####\"\n",
    "    if isinstance(text, str) and delim in text:\n",
    "        return text.split(delim)[1].strip()\n",
    "    return None\n",
    "        \n",
    "\n",
    "text = 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\n'\\\n",
    "    'Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and '\\\n",
    "    'May.\\n'\\\n",
    "    '#### 72'\n",
    "\n",
    "expected = \"72\"\n",
    "res = extract_hash_answer(text)\n",
    "assert res == \"72\", f'not correct, res = {res}'\n",
    "\n",
    "# EXTRACT XML ANSWER\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    import re\n",
    "    match = re.search(r\"<answer>\\s*(.*?)\\s*</answer>\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "sample_text = \"<reasoning>\\nSome reasoning here.\\n</reasoning>\\n<answer>\\nThe answer is 42\\n</answer>\"\n",
    "expected_answer = \"The answer is 42\"\n",
    "extracted = extract_xml_answer(sample_text)\n",
    "assert extracted == expected_answer, f\"extract_xml_answer failed: got {extracted!r}, expected {expected_answer!r}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1df38f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '72',\n",
      " 'prompt': [{'content': '\\n'\n",
      "                        'Respond in the following format:\\n'\n",
      "                        '<reasoning>\\n'\n",
      "                        '...\\n'\n",
      "                        '</reasoning>\\n'\n",
      "                        '<answer>\\n'\n",
      "                        '...\\n'\n",
      "                        '</answer>\\n',\n",
      "             'role': 'system'},\n",
      "            {'content': 'Natalia sold clips to 48 of her friends in April, and '\n",
      "                        'then she sold half as many clips in May. How many '\n",
      "                        'clips did Natalia sell altogether in April and May?',\n",
      "             'role': 'user'}],\n",
      " 'question': 'Natalia sold clips to 48 of her friends in April, and then she '\n",
      "             'sold half as many clips in May. How many clips did Natalia sell '\n",
      "             'altogether in April and May?'}\n"
     ]
    }
   ],
   "source": [
    "gsm8k = dataset.map(lambda x: { # Note that this map function will keep the original features unless overriden\n",
    "    \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": x['question']},\n",
    "    ],\n",
    "    'answer': extract_hash_answer(x['answer'])\n",
    "}) \n",
    "\n",
    "pprint(gsm8k[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82e6b4",
   "metadata": {},
   "source": [
    "# Define reward functions\n",
    "- correctness \n",
    "- integer reward function\n",
    "- strict format func\n",
    "- soft format func\n",
    "- count xml tags <answer>, <reason> , penelize extra tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2f2d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################\n",
      "Question\n",
      "6*7?\n",
      "Answer:\n",
      "42\n",
      "Response:\n",
      "<reasoning>6*7=42.</reasoning><answer>42</answer>\n",
      "Extracted:\n",
      "['42']\n",
      "\n",
      "Test 1: [2.0]\n",
      "#######################\n",
      "Question\n",
      "6*7?\n",
      "Answer:\n",
      "42\n",
      "Response:\n",
      "<reasoning>6*7=42.</reasoning><answer>41</answer>\n",
      "Extracted:\n",
      "['41']\n",
      "\n",
      "Test 2: [0.0]\n",
      "#######################\n",
      "Question\n",
      "6*7?\n",
      "Answer:\n",
      "42\n",
      "Response:\n",
      "<reasoning>6*7=42.</reasoning><answer>42</answer>\n",
      "Extracted:\n",
      "['42', '41']\n",
      "\n",
      "Test 3: [2.0, 0.0]\n",
      "#######################\n",
      "Question\n",
      "6*7?\n",
      "Answer:\n",
      "42\n",
      "Response:\n",
      "<reasoning>6*7=42.</reasoning><answer>42</answer>\n",
      "Extracted:\n",
      "['42']\n",
      "\n",
      "Test 4: [5.0]\n",
      "#######################\n",
      "Question\n",
      "6*7?\n",
      "Answer:\n",
      "42\n",
      "Response:\n",
      "<reasoning>6*7=42.</reasoning>42\n",
      "Extracted:\n",
      "['']\n",
      "\n",
      "Test 5: [0.0]\n"
     ]
    }
   ],
   "source": [
    "def correct_reward_func(prompts, completions, answer, **kwargs):\n",
    "    reward = kwargs.get('reward', 2.0)\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_res = [extract_xml_answer(x) for x in responses]\n",
    "    print(f'#######################\\nQuestion\\n{q}\\nAnswer:\\n{answer[0]}\\nResponse:\\n{responses[0]}\\nExtracted:\\n{extracted_res}\\n')\n",
    "    return [reward if r == a else 0.0 for r,a in zip(extracted_res, answer)]\n",
    "\n",
    "# Unit tests for correct_reward_func using only assert\n",
    "\n",
    "# Test 1: Correct answer\n",
    "test_prompts = [[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"6*7?\"}\n",
    "]]\n",
    "test_completions = [[\n",
    "    {\"role\": \"assistant\", \"content\": \"<reasoning>6*7=42.</reasoning><answer>42</answer>\"}\n",
    "]]\n",
    "test_answer = [\"42\"]\n",
    "test_reward = correct_reward_func(test_prompts, test_completions, test_answer)\n",
    "print(\"Test 1:\", test_reward)\n",
    "assert test_reward == [2.0], f\"Test 1 failed: got {test_reward}, expected [2.0]\"\n",
    "\n",
    "# Test 2: Incorrect answer\n",
    "test_completions_wrong = [[\n",
    "    {\"role\": \"assistant\", \"content\": \"<reasoning>6*7=42.</reasoning><answer>41</answer>\"}\n",
    "]]\n",
    "test_reward_wrong = correct_reward_func(test_prompts, test_completions_wrong, test_answer)\n",
    "print(\"Test 2:\", test_reward_wrong)\n",
    "assert test_reward_wrong == [0.0], f\"Test 2 failed: got {test_reward_wrong}, expected [0.0]\"\n",
    "\n",
    "# Test 3: Multiple completions, mixed correctness\n",
    "test_completions_multi = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"<reasoning>6*7=42.</reasoning><answer>42</answer>\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"<reasoning>6*7=42.</reasoning><answer>41</answer>\"}]\n",
    "]\n",
    "test_answers_multi = [\"42\", \"42\"]\n",
    "test_reward_multi = correct_reward_func(test_prompts, test_completions_multi, test_answers_multi)\n",
    "print(\"Test 3:\", test_reward_multi)\n",
    "assert test_reward_multi == [2.0, 0.0], f\"Test 3 failed: got {test_reward_multi}, expected [2.0, 0.0]\"\n",
    "\n",
    "# Test 4: Custom reward value\n",
    "test_reward_custom = correct_reward_func(test_prompts, test_completions, test_answer, reward=5.0)\n",
    "print(\"Test 4:\", test_reward_custom)\n",
    "assert test_reward_custom == [5.0], f\"Test 4 failed: got {test_reward_custom}, expected [5.0]\"\n",
    "\n",
    "# Test 5: No <answer> tag in completion\n",
    "test_completions_no_tag = [[\n",
    "    {\"role\": \"assistant\", \"content\": \"<reasoning>6*7=42.</reasoning>42\"}\n",
    "]]\n",
    "test_reward_no_tag = correct_reward_func(test_prompts, test_completions_no_tag, test_answer)\n",
    "print(\"Test 5:\", test_reward_no_tag)\n",
    "assert test_reward_no_tag == [0.0], f\"Test 5 failed: got {test_reward_no_tag}, expected [0.0]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af39a146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Pass\n",
      "Test 2: Pass\n",
      "Test 3: Pass\n",
      "Test 4: Pass\n",
      "Test 5: Pass\n"
     ]
    }
   ],
   "source": [
    "# Reward if the model output integer as answer, not float \n",
    "def integer_reward_func(completions, **kwargs):\n",
    "    reward = kwargs.get('reward', 0.5)\n",
    "    responses = [comp[0]['content'] for comp in completions]\n",
    "    extracted_res = [extract_xml_answer(x) for x in responses]\n",
    "    return [reward if r.isdigit() else 0.0 for r in extracted_res]\n",
    "\n",
    "# Unit tests for integer_reward_func\n",
    "test_completions_1 = [[{\"role\": \"assistant\", \"content\": \"<answer>42</answer>\"}]]\n",
    "assert integer_reward_func(test_completions_1) == [0.5], \"Test 1 failed\"\n",
    "print(\"Test 1: Pass\")\n",
    "\n",
    "test_completions_2 = [[{\"role\": \"assistant\", \"content\": \"<answer>abc</answer>\"}]]\n",
    "assert integer_reward_func(test_completions_2) == [0.0], \"Test 2 failed\"\n",
    "print(\"Test 2: Pass\")\n",
    "\n",
    "test_completions_3 = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"<answer>123</answer>\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"<answer>xyz</answer>\"}]\n",
    "]\n",
    "assert integer_reward_func(test_completions_3) == [0.5, 0.0], \"Test 3 failed\"\n",
    "print(\"Test 3: Pass\")\n",
    "\n",
    "test_completions_4 = [[{\"role\": \"assistant\", \"content\": \"<answer>7</answer>\"}]]\n",
    "assert integer_reward_func(test_completions_4, reward=2.0) == [2.0], \"Test 4 failed\"\n",
    "print(\"Test 4: Pass\")\n",
    "\n",
    "test_completions_5 = [[{\"role\": \"assistant\", \"content\": \"<answer></answer>\"}]]\n",
    "assert integer_reward_func(test_completions_5) == [0.0], \"Test 5 failed\"\n",
    "print(\"Test 5: Pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aea21e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strict_format_reward_func Test 1: Pass\n",
      "strict_format_reward_func Test 2: Pass\n",
      "strict_format_reward_func Test 3: Pass\n",
      "strict_format_reward_func Test 4: Pass\n",
      "strict_format_reward_func Test 5: Pass\n"
     ]
    }
   ],
   "source": [
    "def strict_format_reward_func(completions, **kwargs):\n",
    "    reward = kwargs.get(\"reward\", 0.5)\n",
    "    pattern = r\"<reasoning>\\n.*?\\n<reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [reward if match else 0.0 for match in matches]\n",
    "\n",
    "# Unit tests for strict_format_reward_func\n",
    "test_completions_strict_1 = [[{\"role\": \"assistant\", \"content\": \"<reasoning>\\nstep1\\n<reasoning>\\n<answer>\\n42\\n</answer>\\n\"}]]\n",
    "assert strict_format_reward_func(test_completions_strict_1) == [0.5], \"Test 1 failed\"\n",
    "print(\"strict_format_reward_func Test 1: Pass\")\n",
    "\n",
    "test_completions_strict_2 = [[{\"role\": \"assistant\", \"content\": \"<reasoning>step1<reasoning><answer>42</answer>\"}]]\n",
    "assert strict_format_reward_func(test_completions_strict_2) == [0.0], \"Test 2 failed\"\n",
    "print(\"strict_format_reward_func Test 2: Pass\")\n",
    "\n",
    "test_completions_strict_3 = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"<reasoning>\\nfoo\\n<reasoning>\\n<answer>\\nbar\\n</answer>\\n\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"<reasoning>foo<reasoning><answer>bar</answer>\"}]\n",
    "]\n",
    "assert strict_format_reward_func(test_completions_strict_3) == [0.5, 0.0], \"Test 3 failed\"\n",
    "print(\"strict_format_reward_func Test 3: Pass\")\n",
    "\n",
    "test_completions_strict_4 = [[{\"role\": \"assistant\", \"content\": \"<reasoning>\\nstep1\\n<reasoning>\\n<answer>\\n42\\n</answer>\\n\"}]]\n",
    "assert strict_format_reward_func(test_completions_strict_4, reward=2.0) == [2.0], \"Test 4 failed\"\n",
    "print(\"strict_format_reward_func Test 4: Pass\")\n",
    "\n",
    "test_completions_strict_5 = [[{\"role\": \"assistant\", \"content\": \"\"}]]\n",
    "assert strict_format_reward_func(test_completions_strict_5) == [0.0], \"Test 5 failed\"\n",
    "print(\"strict_format_reward_func Test 5: Pass\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e44dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft_format_reward_func Test 1: Pass\n",
      "soft_format_reward_func Test 2: Pass\n",
      "soft_format_reward_func Test 3: Pass\n",
      "soft_format_reward_func Test 4: Pass\n",
      "soft_format_reward_func Test 5: Pass\n"
     ]
    }
   ],
   "source": [
    "def soft_format_reward_func(completions, **kwargs):\n",
    "    reward = kwargs.get(\"reward\", 0.5)\n",
    "    pattern = r\"<reasoning>.*?<reasoning>\\s<answer>.*?</answer>$\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [reward if match else 0.0 for match in matches]\n",
    "\n",
    "# Unit tests for soft_format_reward_func\n",
    "test_completions_soft_1 = [[{\"role\": \"assistant\", \"content\": \"<reasoning>foo<reasoning> <answer>bar</answer>\"}]]\n",
    "assert soft_format_reward_func(test_completions_soft_1) == [0.5], \"Test 1 failed\"\n",
    "print(\"soft_format_reward_func Test 1: Pass\")\n",
    "\n",
    "test_completions_soft_2 = [[{\"role\": \"assistant\", \"content\": \"<reasoning>foo<reasoning><answer>bar</answer>\"}]]\n",
    "assert soft_format_reward_func(test_completions_soft_2) == [0.0], \"Test 2 failed\"\n",
    "print(\"soft_format_reward_func Test 2: Pass\")\n",
    "\n",
    "test_completions_soft_3 = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"<reasoning>abc<reasoning> <answer>xyz</answer>\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"<reasoning>abc<reasoning><answer>xyz</answer>\"}]\n",
    "]\n",
    "assert soft_format_reward_func(test_completions_soft_3) == [0.5, 0.0], \"Test 3 failed\"\n",
    "print(\"soft_format_reward_func Test 3: Pass\")\n",
    "\n",
    "test_completions_soft_4 = [[{\"role\": \"assistant\", \"content\": \"<reasoning>foo<reasoning> <answer>bar</answer>\"}]]\n",
    "assert soft_format_reward_func(test_completions_soft_4, reward=2.0) == [2.0], \"Test 4 failed\"\n",
    "print(\"soft_format_reward_func Test 4: Pass\")\n",
    "\n",
    "test_completions_soft_5 = [[{\"role\": \"assistant\", \"content\": \"\"}]]\n",
    "assert soft_format_reward_func(test_completions_soft_5) == [0.0], \"Test 5 failed\"\n",
    "print(\"soft_format_reward_func Test 5: Pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "705dc105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text:\n",
      "<reasoning>\n",
      "This is my reasoning.\n",
      "</reasoning>\n",
      "<answer>\n",
      "42\n",
      "</answer>\n",
      "\n",
      "Count: 0.496\n",
      "xml_count_reward_func test results: [0.496, 0.25, 0.246, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#reward counts XML tags, penalize extra content\n",
    "def count_xml(text):\n",
    "    count = 0.0\n",
    "\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")) * 0.001 #extra content after /answer is penalized\n",
    "    if text.count(\"\\n</answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")) * 0.001 #extra content after /answer is penalized\n",
    "    return count\n",
    "\n",
    "def xml_count_reward_func(completions, **kwargs):\n",
    "    contents = [completion[0]['content'] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]\n",
    "\n",
    "# Create a sample text to test count_xml\n",
    "sample_text = \"<reasoning>\\nThis is my reasoning.\\n</reasoning>\\n<answer>\\n42\\n</answer>\\n\"\n",
    "count_result = count_xml(sample_text)\n",
    "print(f\"Sample text:\\n{sample_text}\\nCount: {count_result}\")\n",
    "\n",
    "# Quick test for xml_count_reward_func\n",
    "test_completions_xml = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"<reasoning>\\nReasoning here.\\n</reasoning>\\n<answer>\\nAnswer here.\\n</answer>\\n\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"<reasoning>\\nReasoning only.\\n</reasoning>\\n\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"<answer>\\nJust answer.\\n</answer>\\n\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"No tags here.\"}]\n",
    "]\n",
    "\n",
    "xml_counts = xml_count_reward_func(test_completions_xml)\n",
    "print(\"xml_count_reward_func test results:\", xml_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c63f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df694438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd520234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
