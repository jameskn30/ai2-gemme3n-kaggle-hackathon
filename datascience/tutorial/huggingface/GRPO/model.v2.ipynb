{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff06d4a9",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel, FastModel\n",
    "import os\n",
    "from datasets import load_dataset, Dataset, IterableDataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from pprint import pprint\n",
    "import re\n",
    "import wandb\n",
    "from vllm import SamplingParams\n",
    "from transformers import TextStreamer\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Environment variables for torch\n",
    "os.environ[\"TORCH_LOGS\"] = \"recompiles\"\n",
    "os.environ['TORCHDYNAMO_CACHE_SIZE_LIMIT'] = '999999999'\n",
    "import torch._dynamo \n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "log_dir = \"outputs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, \"evaluation.log\")\n",
    "file_handler = logging.FileHandler(log_file, mode='w')  # override the log file\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "# Remove all existing file handlers to ensure override\n",
    "for h in logger.handlers[:]:\n",
    "    if isinstance(h, logging.FileHandler):\n",
    "        logger.removeHandler(h)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get the best available device, with CPU fallback for RTX 5070 Ti\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            # Test if CUDA actually works\n",
    "            test_tensor = torch.randn(1, device='cuda')\n",
    "            print(f\"✅ GPU detected and working: {torch.cuda.get_device_name(0)}\")\n",
    "            return 'cuda'\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  GPU detected but CUDA not compatible: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"   Error: {e}\")\n",
    "            print(\"   Falling back to CPU mode\")\n",
    "            return 'cpu'\n",
    "    else:\n",
    "        print(\"No GPU detected, using CPU\")\n",
    "        return 'cpu'\n",
    "\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2345a6",
   "metadata": {},
   "source": [
    "# Train params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c032669",
   "metadata": {},
   "source": [
    "# Builidng prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ee4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "    f\"\"\"You are given a problem, think about the problem and provide your workout. \n",
    "    Place it between {reasoning_start} and {reasoning_end}. Then provide your solution\n",
    "    between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "print(system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3cc28",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('openai/gsm8k', 'main', split='train')\n",
    "print(dataset)\n",
    "dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866fc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text: return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "pprint(dataset[0])\n",
    "assert int(dataset[0]['answer']), \"answer not a number format\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07e9ba",
   "metadata": {},
   "source": [
    "# Format match function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This regular expression is used to match a specific format in a string, typically for extracting\n",
    "# the solution part from a text that contains both reasoning and solution sections.\n",
    "# - It expects the string to start with optional whitespace.\n",
    "# - Then it looks for the reasoning section, which starts with the value of `reasoning_start`,\n",
    "#   contains any characters (non-greedy), and ends with `reasoning_end`.\n",
    "# - After that, it expects the solution section, which starts with `solution_start`,\n",
    "#   captures everything up to `solution_end` (the solution itself is captured in a group).\n",
    "# - Finally, it expects optional whitespace at the end of the string.\n",
    "# - The flags `re.MULTILINE` and `re.DOTALL` allow the regex to match across multiple lines\n",
    "#   and let the dot (`.`) match newline characters as well.\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "#test\n",
    "res = match_format.search(\n",
    "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
    "    \"<SOLUTION>2</SOLUTION>\",\n",
    ")\n",
    "\n",
    "print(res)\n",
    "print(res.group(1))\n",
    "\n",
    "res = match_format.search(\n",
    "    \"<SOLUTION>2</SOLUTION>\",\n",
    ")\n",
    "\n",
    "print(res)\n",
    "# print(res.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        res = completion[0]['content']\n",
    "        if match_format.search(res) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# Unit tests for match_format_exactly\n",
    "actual = match_format_exactly([[{'content': \"<start_working_out>Reason<end_working_out><SOLUTION>42</SOLUTION>\"}]])\n",
    "assert actual == [3.0], f\"Test 1 Failed: Expected [3.0] for valid reasoning and solution, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"<start_working_out>R<end_working_out><SOLUTION>ans</SOLUTION>\"}]])\n",
    "assert actual == [3.0], f\"Test 2 Failed: Expected [3.0] for valid short reasoning and solution, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"<start_working_out>R<end_working_out><SOLUTION></SOLUTION>\"}]])\n",
    "assert actual == [0.0], f\"Test 3 Failed: Expected [0.0] for empty solution but valid format, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"<start_working_out>R<end_working_out>\"}]])\n",
    "assert actual == [0], f\"Test 4 Failed: Expected [0] for missing solution section, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"<SOLUTION>42</SOLUTION>\"}]])\n",
    "assert actual == [0], f\"Test 5 Failed: Expected [0] for missing reasoning section, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"\"}]])\n",
    "assert actual == [0], f\"Test 6 Failed: Expected [0] for empty string, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"<start_working_out>R<end_working_out><SOLUTION>ans</SOLUTION> extra\"}]])\n",
    "assert actual == [0], f\"Test 7 Failed: Expected [0] for extra text after valid format, got {actual}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11097947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_approx(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0]['content']\n",
    "        scores.append(\n",
    "            sum(0.5 if response.count(tag) == 1 else -0.5 \n",
    "                for tag in [reasoning_start, reasoning_end, solution_start, solution_end])\n",
    "        )\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Unit tests for match_format_approx\n",
    "actual = match_format_approx([[{'content': \"<start_working_out>R<end_working_out><SOLUTION>42</SOLUTION>\"}]])\n",
    "assert actual == [2.0], f\"Test 1 Failed: Expected [2.0] for all tags present once, got {actual}\"\n",
    "\n",
    "actual = match_format_approx([[{'content': \"<start_working_out>R<end_working_out>\"}]])\n",
    "assert actual == [0.0], f\"Test 2 Failed: Expected [0.0] for only reasoning tags, got {actual}\"\n",
    "\n",
    "actual = match_format_approx([[{'content': \"<SOLUTION>42</SOLUTION>\"}]])\n",
    "assert actual == [0.0], f\"Test 3 Failed: Expected [0.0] for only solution tags, got {actual}\"\n",
    "\n",
    "actual = match_format_approx([[{'content': \"<start_working_out>R<end_working_out><SOLUTION>42</SOLUTION> extra\"}]])\n",
    "assert actual == [2.0], f\"Test 4 Failed: Expected [2.0] for all tags present with extra text, got {actual}\"\n",
    "\n",
    "actual = match_format_approx([[{'content': \"\"}]])\n",
    "assert actual == [-2.0], f\"Test 5 Failed: Expected [-2.0] for missing all tags, got {actual}\"\n",
    "\n",
    "actual = match_format_approx([[{'content': \"<start_working_out>R<end_working_out><SOLUTION>42</SOLUTION><SOLUTION>43</SOLUTION>\"}]])\n",
    "assert actual == [0.0], f\"Test 6 Failed: Expected [0.0] for duplicate solution tags, got {actual}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf36fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1) #Answer within <Solution> \n",
    "        if(guess:= match_format.search(res)) is not None else None\n",
    "        for res in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        elif guess.strip() == true_answer.strip(): # correct answer but there are spaces in between won't get full points\n",
    "            score += 1.5\n",
    "        else:\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if 0.9 <= ratio <= 1.1:\n",
    "                    score += 0.5\n",
    "                elif 0.8 <= ratio <= 1.2:\n",
    "                    score += 0.25\n",
    "                else:\n",
    "                    score -= 1.0 #wrong answer, penalize\n",
    "            except:\n",
    "                score -= 0.5 #unknown format \n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Compact unit tests for check_answer\n",
    "\n",
    "# Test 1: Exact match\n",
    "\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Some reasoning here{reasoning_end}<SOLUTION>42</SOLUTION>\"}]], [\"42\"])\n",
    "assert actual == [3.0], f\"Test 1 Failed: {actual}\"\n",
    "\n",
    "# Test 2: Whitespace match\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Reasoning{reasoning_end}<SOLUTION>   42  </SOLUTION>\"}]], [\"42\"])\n",
    "assert actual == [1.5], f\"Test 2 Failed: {actual}\"\n",
    "\n",
    "# Test 3: Ratio within 10%\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Math steps{reasoning_end}<SOLUTION>95</SOLUTION>\"}]], [\"100\"])\n",
    "assert actual == [0.5], f\"Test 3 Failed: {actual}\"\n",
    "\n",
    "# Test 4: Ratio within 20%\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Estimate{reasoning_end}<SOLUTION>85</SOLUTION>\"}]], [\"100\"])\n",
    "assert actual == [0.25], f\"Test 4 Failed: {actual}\"\n",
    "\n",
    "# Test 5: Wrong numeric answer\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Wrong math{reasoning_end}<SOLUTION>50</SOLUTION>\"}]], [\"100\"])\n",
    "assert actual == [-1.0], f\"Test 5 Failed: {actual}\"\n",
    "\n",
    "# Test 6: Unknown format\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Nonsense{reasoning_end}<SOLUTION>foo</SOLUTION>\"}]], [\"100\"])\n",
    "assert actual == [-0.5], f\"Test 6 Failed: {actual}\"\n",
    "\n",
    "# Test 7: No match\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}No answer here{reasoning_end}\"}]], [\"100\"])\n",
    "assert actual == [0], f\"Test 7 Failed: {actual}\"\n",
    "\n",
    "# Test 8: Multiple answers\n",
    "actual = check_answer(\n",
    "    [[\"Q1\"], [\"Q2\"]],\n",
    "    [\n",
    "        [{'content': f\"{reasoning_start}Reasoning1{reasoning_end}<SOLUTION>4</SOLUTION>\"}],\n",
    "        [{'content': f\"{reasoning_start}Reasoning2{reasoning_end}<SOLUTION>6</SOLUTION>\"}]\n",
    "    ],\n",
    "    [\"4\", \"6\"]\n",
    ")\n",
    "assert actual == [3.0, 3.0], f\"Test 8 Failed: {actual}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_numbers.findall(\"<SOLUTION>  Answer is 0.34, Another answer is 0.45 </SOLUTION>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e9e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "You are given a problem, think about the problem and provide your workout. \n",
    "    Place it between <start_working_out> and <end_working_out>. Then provide your solution\n",
    "    between <SOLUTION></SOLUTION>\n",
    "\n",
    "A wooden bridge can carry no more than 5000 pounds. A delivery truck filled with identical boxes, each weighing 15 pounds, will pass over the bridge. The combined weight of the driver and the empty truck is 3755 pounds. What is the maximum number of boxes which can be loaded onto the truck while not exceeding the bridge's weight limit?<end_of_turn>\n",
    "<start_of_turn>model\n",
    "<SOLUTION>125</SOLUTION>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extract_solution_content(text):\n",
    "#     \"\"\"\n",
    "#     Extracts the content strictly between <SOLUTION> and </SOLUTION> tags (case-insensitive).\n",
    "#     Returns the matched string or None if not found.\n",
    "#     \"\"\"\n",
    "#     match = re.search(r\"<SOLUTION>(.*?)</SOLUTION>\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "#     return match.group(1).strip() if match else None\n",
    "\n",
    "# # Example usage:\n",
    "# example_text = \"<SOLUTION>  Answer is 0.34, Another answer is 0.45 </SOLUTION>\"\n",
    "# solution_content = extract_solution_content(example_text)\n",
    "# print(solution_content)  # Output: Answer is 0.34, Another answer is 0.45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response(response):\n",
    "    guess = match_numbers.search(response)\n",
    "    return guess.group(1) if guess != None else None\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1]['content']\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        res\n",
    "        if (res := extract_response(response)) is not None else None\n",
    "        for response in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    print('*'*20, f\"\\nQuestion:\\n{question}\", f\"Answer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        \n",
    "        if guess is None:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        \n",
    "        except:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test 1: Correct extraction and matching\n",
    "actual = check_numbers(\n",
    "    [[{'content': ''}]],\n",
    "    [[{'content': '<SOLUTION> 0.34 </SOLUTION>'}]],\n",
    "    [\"0.34\"]\n",
    ")\n",
    "assert actual == [1.5], f\"Test 1 Failed: {actual}\"\n",
    "\n",
    "# Test 2: Extraction with extra text\n",
    "actual = check_numbers(\n",
    "    [[{'content': ''}]],\n",
    "    [[{'content': '<SOLUTION> Answer is 0.34, Another answer is 0.45 </SOLUTION>'}]],\n",
    "    [\"0.34\"]\n",
    ")\n",
    "assert actual == [1.5], f\"Test 2 Failed: {actual}\"\n",
    "\n",
    "# Test 3: Incorrect number\n",
    "actual = check_numbers(\n",
    "    [[{'content': ''}]],\n",
    "    [[{'content': '<SOLUTION> 0.45 </SOLUTION>'}]],\n",
    "    [\"0.34\"]\n",
    ")\n",
    "assert actual == [0.0], f\"Test 3 Failed: {actual}\"\n",
    "\n",
    "# Test 4: No number found\n",
    "actual = check_numbers(\n",
    "    [[{'content': ''}]],\n",
    "    [[{'content': '<SOLUTION> no number here </SOLUTION>'}]],\n",
    "    [\"0.34\"]\n",
    ")\n",
    "assert actual == [0.0], f\"Test 4 Failed: {actual}\"\n",
    "\n",
    "# Test 5: Multiple completions\n",
    "actual = check_numbers(\n",
    "    [[{'content': ''}, {'content': ''}]],\n",
    "    [[{'content': '<SOLUTION> 1.23 </SOLUTION>'}], [{'content': '<SOLUTION> 4.56 </SOLUTION>'}]],\n",
    "    [\"1.23\", \"4.56\"]\n",
    ")\n",
    "assert actual == [1.5, 1.5], f\"Test 5 Failed: {actual}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f63ea",
   "metadata": {},
   "source": [
    "# GRPOConfig and GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    max_seq_length = 1024\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name = \"unsloth/gemma-3-1b-it\",\n",
    "        max_seq_length = max_seq_length, # Choose any for long context!\n",
    "        load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "        load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "        full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    )\n",
    "\n",
    "    model = FastModel.get_peft_model(\n",
    "        model,\n",
    "        finetune_vision_layers     = False, # Turn off for just text!\n",
    "        finetune_language_layers   = True,  # Should leave on!\n",
    "        finetune_attention_modules = True,  # Attention good for GRPO\n",
    "        finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "        r = 8,           # Larger = higher accuracy, but might overfit\n",
    "        lora_alpha = 8,  # Recommended alpha == r at least\n",
    "        lora_dropout = 0,\n",
    "        bias = \"none\",\n",
    "        random_state = 3407,\n",
    "    )\n",
    "    print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    max_prompt_length = 256\n",
    "    max_seq_length = 1024\n",
    "\n",
    "    grpo_config = GRPOConfig(\n",
    "        learning_rate = 5e-6,                # The initial learning rate for the optimizer\n",
    "        adam_beta1 = 0.9,                    # Beta1 parameter for Adam optimizer (exponential decay rate for first moment estimates)\n",
    "        adam_beta2 = 0.99,                   # Beta2 parameter for Adam optimizer (exponential decay rate for second moment estimates)\n",
    "        weight_decay = 0.1,                  # Weight decay (L2 penalty) to prevent overfitting\n",
    "        warmup_ratio = 0.1,                  # Fraction of total steps used for learning rate warmup\n",
    "        lr_scheduler_type = \"cosine\",        # Type of learning rate scheduler (\"cosine\" annealing)\n",
    "        optim = \"adamw_torch_fused\",         # Optimizer type (fused AdamW for efficiency)\n",
    "        logging_steps = 1,                   # Log training metrics every N steps\n",
    "        per_device_train_batch_size = 1,     # Batch size per device (GPU/CPU) during training\n",
    "        gradient_accumulation_steps = 1,     # Number of steps to accumulate gradients before updating weights (increase for larger effective batch size)\n",
    "        num_generations = 4,                 # Number of generations per prompt (reduce if out of memory)\n",
    "        max_prompt_length = max_prompt_length,                   # Maximum length of the input prompt\n",
    "        max_completion_length = max_seq_length - max_prompt_length, # Maximum length of the generated completion\n",
    "        num_train_epochs = 1,                # Number of training epochs (uncomment and set for full training run)\n",
    "        # max_steps = 50,                      # Total number of training steps\n",
    "        save_steps = 50,                     # Save checkpoint every N steps\n",
    "        max_grad_norm = 0.1,                 # Maximum gradient norm for gradient clipping\n",
    "        report_to = \"none\",                  # Reporting backend (\"none\" disables reporting, can use \"wandb\" for Weights & Biases)\n",
    "        output_dir = \"outputs\"               # Directory to save model checkpoints and outputs\n",
    "        \n",
    "    )\n",
    "\n",
    "    trainer = GRPOTrainer(\n",
    "        model = model,\n",
    "        processing_class = tokenizer,\n",
    "        reward_funcs = [\n",
    "            match_format_exactly,\n",
    "            match_format_approx,\n",
    "            check_answer,\n",
    "            check_numbers,\n",
    "        ],\n",
    "        args = grpo_config,\n",
    "        train_dataset = dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained('outputs/gemma-3-tune1')\n",
    "    tokenizer.save_pretrained('outputs/gemma-3-tune1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ec8d7",
   "metadata": {},
   "source": [
    "# Evaluate pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26709f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model weights\n",
    "# model.save_pretrained('outputs/gemma-3-tune1')\n",
    "model, tokenizer = FastModel.from_pretrained('outputs/gemma-3-tune1')\n",
    "print(type(model))\n",
    "print(type(tokenizer))\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample inference\n",
    "messages = [\n",
    "    {'role': 'system', \"content\": system_prompt},\n",
    "    {'role': 'user', \"content\": \"What is the square root of 1010?\"},\n",
    "]\n",
    "\n",
    "token_ids = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize=True\n",
    ").to('cuda')\n",
    "\n",
    "print(token_ids)\n",
    "print(type(token_ids))\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "\n",
    "output = model.generate(\n",
    "    token_ids,\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    # streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")\n",
    "\n",
    "sample = tokenizer.decode(output[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sample_completion = '''\n",
    "You are given a problem, think about the problem and provide your workout. \n",
    "    Place it between <start_working_out> and <end_working_out>. Then provide your solution\n",
    "    between <SOLUTION></SOLUTION>\n",
    "\n",
    "A wooden bridge can carry no more than 5000 pounds. A delivery truck filled with identical boxes, each weighing 15 pounds, will pass over the bridge. The combined weight of the driver and the empty truck is 3755 pounds. What is the maximum number of boxes which can be loaded onto the truck while not exceeding the bridge's weight limit?<end_of_turn>\n",
    "<start_working_out>model<end_working_out>\n",
    "<SOLUTION>125</SOLUTION>\n",
    "'''\n",
    "\n",
    "# Regex to extract the number within <SOLUTION>...</SOLUTION>\n",
    "# match_solution = re.compile(r\"<SOLUTION>\\s*([\\d\\.]+)\\s*</SOLUTION>\", re.IGNORECASE)\n",
    "\n",
    "# def get_solution(completion):\n",
    "#     return guess.group(1) if (guess := match_format.search(completion)) is not None else None\n",
    "\n",
    "# print(get_solution(sample_completion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc06fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = load_dataset('openai/gsm8k', 'main', split=\"test\")\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, question, **kwargs):\n",
    "    MAX_NEW_TOKENS = kwargs.get(\"max_new_tokens\", 64)\n",
    "\n",
    "    # Sample inference\n",
    "    messages = [\n",
    "        {'role': 'system', \"content\": system_prompt},\n",
    "        {'role': 'user', \"content\": question},\n",
    "    ]\n",
    "\n",
    "    token_ids = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt = True,\n",
    "        return_tensors = \"pt\",\n",
    "        tokenize=True\n",
    "    ).to('cuda')\n",
    "\n",
    "    # from transformers import TextStreamer\n",
    "\n",
    "    output = model.generate(\n",
    "        token_ids,\n",
    "        max_new_tokens = MAX_NEW_TOKENS, # Increase for longer outputs!\n",
    "        # Recommended Gemma-3 settings!\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    )\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_answer_from_completion(output: torch.Tensor):\n",
    "    assert type(output) == torch.Tensor, 'Wrong type: output type must be torch.tensor'\n",
    "\n",
    "    completion = tokenizer.decode(output[0])\n",
    "    return guess.group(1) if (guess:= match_format.search(completion)) is not None else None, completion\n",
    "\n",
    "\n",
    "logging.info(\"##### EVALUATION #####\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "correct_answer = 0\n",
    "total = 0 \n",
    "loop = tqdm(test_ds, desc=\"Evaluating\")\n",
    "for X in loop:\n",
    "    total += 1\n",
    "    try:\n",
    "        question = X[\"question\"]\n",
    "        answer = extract_hash_answer(X['answer'])\n",
    "        y = generate(model, question, max_new_tokens = 1024)\n",
    "        pred_answer, completion = get_answer_from_completion(y)\n",
    "\n",
    "        # Log for later analysis\n",
    "\n",
    "        logger.info('question\\t' + question)\n",
    "        logger.info('target_answer\\t'+  answer)\n",
    "        logger.info('pred_answer\\t'+ (pred_answer if pred_answer else 'Wrong format!!'))\n",
    "        logger.info('completion\\t'+ completion)\n",
    "\n",
    "        correct_answer += 1 if (pred_answer != None and pred_answer == answer) else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in this question \", question)\n",
    "        logger.errror(e)\n",
    "    \n",
    "    accuracy = (correct_answer / total) * 100\n",
    "    logger.info(f'Accuracy = {accuracy:.2f}')\n",
    "    logger.info(\"################################################\")\n",
    "    loop.set_description(f\"accuracy = {accuracy}\")\n",
    "\n",
    "# generate(model, test_ds[0]['question'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_log = {'loss': 0.0, 'grad_norm': 0.25990259647369385, 'learning_rate': 0.0, 'num_tokens': 2137.0, 'completions/mean_length': 433.25, 'completions/min_length': 237.0, 'completions/max_length': 768.0, 'completions/clipped_ratio': 0.25, 'completions/mean_terminated_length': 321.66668701171875, 'completions/min_terminated_length': 237.0, 'completions/max_terminated_length': 467.0, 'rewards/match_format_exactly/mean': 0.0, 'rewards/match_format_exactly/std': 0.0, 'rewards/match_format_approx/mean': 0.25, 'rewards/match_format_approx/std': 0.9574271440505981, 'rewards/check_answer/mean': 0.0, 'rewards/check_answer/std': 0.0, 'rewards/check_numbers/mean': 0.375, 'rewards/check_numbers/std': 0.75, 'reward': 0.625, 'reward_std': 1.4930394887924194, 'frac_reward_zero_std': 0.0, 'completion_length': 433.25, 'kl': 0.0, 'epoch': 0.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528e7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8fafa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
