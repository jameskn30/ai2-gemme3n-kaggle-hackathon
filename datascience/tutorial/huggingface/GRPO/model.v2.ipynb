{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff06d4a9",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5873c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen/Desktop/ai2-gemme3n-kaggle-hackathon/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-24 21:19:31 [__init__.py:244] Automatically detected platform cuda.\n",
      "GPU detected: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel, FastModel\n",
    "import os\n",
    "from datasets import load_dataset, Dataset, IterableDataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from pprint import pprint\n",
    "import re\n",
    "import wandb\n",
    "from vllm import SamplingParams\n",
    "from transformers import TextStreamer\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n",
    "\n",
    "# Environment variables for torch\n",
    "os.environ[\"TORCH_LOGS\"] = \"recompiles\"\n",
    "os.environ['TORCHDYNAMO_CACHE_SIZE_LIMIT'] = '999999999'\n",
    "import torch._dynamo \n",
    "torch._dynamo.config.cache_size_limit = 64\n",
    "\n",
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "log_dir = \"outputs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, \"evaluation.log\")\n",
    "file_handler = logging.FileHandler(log_file, mode='w')  # override the log file\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "# Remove all existing file handlers to ensure override\n",
    "for h in logger.handlers[:]:\n",
    "    if isinstance(h, logging.FileHandler):\n",
    "        logger.removeHandler(h)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49ee4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a problem, think about the problem and provide your workout. \n",
      "    Place it between <start_working_out> and <end_working_out>. Then provide your solution\n",
      "    between <SOLUTION></SOLUTION>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "    f\"\"\"You are given a problem, think about the problem and provide your workout. \n",
    "    Place it between {reasoning_start} and {reasoning_end}. Then provide your solution\n",
    "    between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "print(system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3cc28",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8c9e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 7473\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('openai/gsm8k', 'main', split='train')\n",
    "print(dataset)\n",
    "dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866fc145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'prompt'],\n",
      "    num_rows: 7473\n",
      "})\n",
      "{'answer': '72',\n",
      " 'prompt': [{'content': 'You are given a problem, think about the problem and '\n",
      "                        'provide your workout. \\n'\n",
      "                        '    Place it between <start_working_out> and '\n",
      "                        '<end_working_out>. Then provide your solution\\n'\n",
      "                        '    between <SOLUTION></SOLUTION>',\n",
      "             'role': 'system'},\n",
      "            {'content': 'Natalia sold clips to 48 of her friends in April, and '\n",
      "                        'then she sold half as many clips in May. How many '\n",
      "                        'clips did Natalia sell altogether in April and May?',\n",
      "             'role': 'user'}],\n",
      " 'question': 'Natalia sold clips to 48 of her friends in April, and then she '\n",
      "             'sold half as many clips in May. How many clips did Natalia sell '\n",
      "             'altogether in April and May?'}\n"
     ]
    }
   ],
   "source": [
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text: return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "pprint(dataset[0])\n",
    "assert int(dataset[0]['answer']), \"answer not a number format\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07e9ba",
   "metadata": {},
   "source": [
    "# Format match function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d866da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 71), match='<start_working_out>Let me think!<end_working_out>>\n",
      "2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# This regular expression is used to match a specific format in a string, typically for extracting\n",
    "# the solution part from a text that contains both reasoning and solution sections.\n",
    "# - It expects the string to start with optional whitespace.\n",
    "# - Then it looks for the reasoning section, which starts with the value of `reasoning_start`,\n",
    "#   contains any characters (non-greedy), and ends with `reasoning_end`.\n",
    "# - After that, it expects the solution section, which starts with `solution_start`,\n",
    "#   captures everything up to `solution_end` (the solution itself is captured in a group).\n",
    "# - Finally, it expects optional whitespace at the end of the string.\n",
    "# - The flags `re.MULTILINE` and `re.DOTALL` allow the regex to match across multiple lines\n",
    "#   and let the dot (`.`) match newline characters as well.\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "#test\n",
    "res = match_format.search(\n",
    "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
    "    \"<SOLUTION>2</SOLUTION>\",\n",
    ")\n",
    "\n",
    "print(res)\n",
    "print(res.group(1))\n",
    "\n",
    "res = match_format.search(\n",
    "    \"<SOLUTION>2</SOLUTION>\",\n",
    ")\n",
    "\n",
    "print(res)\n",
    "# print(res.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0248e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        res = completion[0]['content']\n",
    "        if match_format.search(res) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "# Unit tests for match_format_exactly\n",
    "actual = match_format_exactly([[{'content': \"<start_working_out>Reason<end_working_out><SOLUTION>42</SOLUTION>\"}]])\n",
    "assert actual == [3.0], f\"Test 1 Failed: Expected [3.0] for valid reasoning and solution, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"<start_working_out>R<end_working_out><SOLUTION>ans</SOLUTION>\"}]])\n",
    "assert actual == [3.0], f\"Test 2 Failed: Expected [3.0] for valid short reasoning and solution, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"<start_working_out>R<end_working_out><SOLUTION></SOLUTION>\"}]])\n",
    "assert actual == [0.0], f\"Test 3 Failed: Expected [0.0] for empty solution but valid format, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"<start_working_out>R<end_working_out>\"}]])\n",
    "assert actual == [0], f\"Test 4 Failed: Expected [0] for missing solution section, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"<SOLUTION>42</SOLUTION>\"}]])\n",
    "assert actual == [0], f\"Test 5 Failed: Expected [0] for missing reasoning section, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"\"}]])\n",
    "assert actual == [0], f\"Test 6 Failed: Expected [0] for empty string, got {actual}\"\n",
    "\n",
    "actual = match_format_exactly([[{'content': \"<start_working_out>R<end_working_out><SOLUTION>ans</SOLUTION> extra\"}]])\n",
    "assert actual == [0], f\"Test 7 Failed: Expected [0] for extra text after valid format, got {actual}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11097947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_approx(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0]['content']\n",
    "        scores.append(\n",
    "            sum(0.5 if response.count(tag) == 1 else -0.5 \n",
    "                for tag in [reasoning_start, reasoning_end, solution_start, solution_end])\n",
    "        )\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Unit tests for match_format_approx\n",
    "actual = match_format_approx([[{'content': \"<start_working_out>R<end_working_out><SOLUTION>42</SOLUTION>\"}]])\n",
    "assert actual == [2.0], f\"Test 1 Failed: Expected [2.0] for all tags present once, got {actual}\"\n",
    "\n",
    "actual = match_format_approx([[{'content': \"<start_working_out>R<end_working_out>\"}]])\n",
    "assert actual == [0.0], f\"Test 2 Failed: Expected [0.0] for only reasoning tags, got {actual}\"\n",
    "\n",
    "actual = match_format_approx([[{'content': \"<SOLUTION>42</SOLUTION>\"}]])\n",
    "assert actual == [0.0], f\"Test 3 Failed: Expected [0.0] for only solution tags, got {actual}\"\n",
    "\n",
    "actual = match_format_approx([[{'content': \"<start_working_out>R<end_working_out><SOLUTION>42</SOLUTION> extra\"}]])\n",
    "assert actual == [2.0], f\"Test 4 Failed: Expected [2.0] for all tags present with extra text, got {actual}\"\n",
    "\n",
    "actual = match_format_approx([[{'content': \"\"}]])\n",
    "assert actual == [-2.0], f\"Test 5 Failed: Expected [-2.0] for missing all tags, got {actual}\"\n",
    "\n",
    "actual = match_format_approx([[{'content': \"<start_working_out>R<end_working_out><SOLUTION>42</SOLUTION><SOLUTION>43</SOLUTION>\"}]])\n",
    "assert actual == [0.0], f\"Test 6 Failed: Expected [0.0] for duplicate solution tags, got {actual}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf36fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1) # the answer after hash #### \n",
    "        if(guess:= match_format.search(res)) is not None else None\n",
    "        for res in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        elif guess.strip() == true_answer.strip(): # correct answer but there are spaces in between won't get full points\n",
    "            score += 1.5\n",
    "        else:\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if 0.9 <= ratio <= 1.1:\n",
    "                    score += 0.5\n",
    "                elif 0.8 <= ratio <= 1.2:\n",
    "                    score += 0.25\n",
    "                else:\n",
    "                    score -= 1.0 #wrong answer, penalize\n",
    "            except:\n",
    "                score -= 0.5 #unknown format \n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Compact unit tests for check_answer\n",
    "\n",
    "# Test 1: Exact match\n",
    "\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Some reasoning here{reasoning_end}<SOLUTION>42</SOLUTION>\"}]], [\"42\"])\n",
    "assert actual == [3.0], f\"Test 1 Failed: {actual}\"\n",
    "\n",
    "# Test 2: Whitespace match\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Reasoning{reasoning_end}<SOLUTION>   42  </SOLUTION>\"}]], [\"42\"])\n",
    "assert actual == [1.5], f\"Test 2 Failed: {actual}\"\n",
    "\n",
    "# Test 3: Ratio within 10%\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Math steps{reasoning_end}<SOLUTION>95</SOLUTION>\"}]], [\"100\"])\n",
    "assert actual == [0.5], f\"Test 3 Failed: {actual}\"\n",
    "\n",
    "# Test 4: Ratio within 20%\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Estimate{reasoning_end}<SOLUTION>85</SOLUTION>\"}]], [\"100\"])\n",
    "assert actual == [0.25], f\"Test 4 Failed: {actual}\"\n",
    "\n",
    "# Test 5: Wrong numeric answer\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Wrong math{reasoning_end}<SOLUTION>50</SOLUTION>\"}]], [\"100\"])\n",
    "assert actual == [-1.0], f\"Test 5 Failed: {actual}\"\n",
    "\n",
    "# Test 6: Unknown format\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}Nonsense{reasoning_end}<SOLUTION>foo</SOLUTION>\"}]], [\"100\"])\n",
    "assert actual == [-0.5], f\"Test 6 Failed: {actual}\"\n",
    "\n",
    "# Test 7: No match\n",
    "actual = check_answer([[\"Q\"]], [[{'content': f\"{reasoning_start}No answer here{reasoning_end}\"}]], [\"100\"])\n",
    "assert actual == [0], f\"Test 7 Failed: {actual}\"\n",
    "\n",
    "# Test 8: Multiple answers\n",
    "actual = check_answer(\n",
    "    [[\"Q1\"], [\"Q2\"]],\n",
    "    [\n",
    "        [{'content': f\"{reasoning_start}Reasoning1{reasoning_end}<SOLUTION>4</SOLUTION>\"}],\n",
    "        [{'content': f\"{reasoning_start}Reasoning2{reasoning_end}<SOLUTION>6</SOLUTION>\"}]\n",
    "    ],\n",
    "    [\"4\", \"6\"]\n",
    ")\n",
    "assert actual == [3.0, 3.0], f\"Test 8 Failed: {actual}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7501452a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.34']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_numbers.findall(\"<SOLUTION>  Answer is 0.34, Another answer is 0.45 </SOLUTION>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a21d73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Question:\n",
      " Answer:\n",
      "0.34 \n",
      "Response:\n",
      "<SOLUTION> 0.34 </SOLUTION> \n",
      "Extracted:\n",
      "0.34\n",
      "******************** Question:\n",
      " Answer:\n",
      "0.34 \n",
      "Response:\n",
      "<SOLUTION> Answer is 0.34, Another answer is 0.45 </SOLUTION> \n",
      "Extracted:\n",
      "0.34\n",
      "******************** Question:\n",
      " Answer:\n",
      "0.34 \n",
      "Response:\n",
      "<SOLUTION> 0.45 </SOLUTION> \n",
      "Extracted:\n",
      "0.45\n",
      "******************** Question:\n",
      " Answer:\n",
      "0.34 \n",
      "Response:\n",
      "<SOLUTION> no number here </SOLUTION> \n",
      "Extracted:\n",
      "None\n",
      "******************** Question:\n",
      " Answer:\n",
      "1.23 \n",
      "Response:\n",
      "<SOLUTION> 1.23 </SOLUTION> \n",
      "Extracted:\n",
      "1.23\n"
     ]
    }
   ],
   "source": [
    "def extract_response(response):\n",
    "    guess = match_numbers.search(response)\n",
    "    return guess.group(1) if guess != None else None\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1]['content']\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        # guess.group(1)\n",
    "        result\n",
    "        if (result := extract_response(response)) is not None else None\n",
    "        for response in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    print('*'*20, f\"Question:\\n{question}\", f\"Answer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        \n",
    "        if guess is None:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        \n",
    "        except:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test 1: Correct extraction and matching\n",
    "actual = check_numbers(\n",
    "    [[{'content': ''}]],\n",
    "    [[{'content': '<SOLUTION> 0.34 </SOLUTION>'}]],\n",
    "    [\"0.34\"]\n",
    ")\n",
    "assert actual == [1.5], f\"Test 1 Failed: {actual}\"\n",
    "\n",
    "# Test 2: Extraction with extra text\n",
    "actual = check_numbers(\n",
    "    [[{'content': ''}]],\n",
    "    [[{'content': '<SOLUTION> Answer is 0.34, Another answer is 0.45 </SOLUTION>'}]],\n",
    "    [\"0.34\"]\n",
    ")\n",
    "assert actual == [1.5], f\"Test 2 Failed: {actual}\"\n",
    "\n",
    "# Test 3: Incorrect number\n",
    "actual = check_numbers(\n",
    "    [[{'content': ''}]],\n",
    "    [[{'content': '<SOLUTION> 0.45 </SOLUTION>'}]],\n",
    "    [\"0.34\"]\n",
    ")\n",
    "assert actual == [0.0], f\"Test 3 Failed: {actual}\"\n",
    "\n",
    "# Test 4: No number found\n",
    "actual = check_numbers(\n",
    "    [[{'content': ''}]],\n",
    "    [[{'content': '<SOLUTION> no number here </SOLUTION>'}]],\n",
    "    [\"0.34\"]\n",
    ")\n",
    "assert actual == [0.0], f\"Test 4 Failed: {actual}\"\n",
    "\n",
    "# Test 5: Multiple completions\n",
    "actual = check_numbers(\n",
    "    [[{'content': ''}, {'content': ''}]],\n",
    "    [[{'content': '<SOLUTION> 1.23 </SOLUTION>'}], [{'content': '<SOLUTION> 4.56 </SOLUTION>'}]],\n",
    "    [\"1.23\", \"4.56\"]\n",
    ")\n",
    "assert actual == [1.5, 1.5], f\"Test 5 Failed: {actual}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f63ea",
   "metadata": {},
   "source": [
    "# GRPOConfig and GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ec2c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    max_seq_length = 1024\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name = \"unsloth/gemma-3-1b-it\",\n",
    "        max_seq_length = max_seq_length, # Choose any for long context!\n",
    "        load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "        load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "        full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    )\n",
    "\n",
    "    model = FastModel.get_peft_model(\n",
    "        model,\n",
    "        finetune_vision_layers     = False, # Turn off for just text!\n",
    "        finetune_language_layers   = True,  # Should leave on!\n",
    "        finetune_attention_modules = True,  # Attention good for GRPO\n",
    "        finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "        r = 8,           # Larger = higher accuracy, but might overfit\n",
    "        lora_alpha = 8,  # Recommended alpha == r at least\n",
    "        lora_dropout = 0,\n",
    "        bias = \"none\",\n",
    "        random_state = 3407,\n",
    "    )\n",
    "    print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6ce203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    max_prompt_length = 256\n",
    "    max_seq_length = 1024\n",
    "\n",
    "    grpo_config = GRPOConfig(\n",
    "        learning_rate = 5e-6,                # The initial learning rate for the optimizer\n",
    "        adam_beta1 = 0.9,                    # Beta1 parameter for Adam optimizer (exponential decay rate for first moment estimates)\n",
    "        adam_beta2 = 0.99,                   # Beta2 parameter for Adam optimizer (exponential decay rate for second moment estimates)\n",
    "        weight_decay = 0.1,                  # Weight decay (L2 penalty) to prevent overfitting\n",
    "        warmup_ratio = 0.1,                  # Fraction of total steps used for learning rate warmup\n",
    "        lr_scheduler_type = \"cosine\",        # Type of learning rate scheduler (\"cosine\" annealing)\n",
    "        optim = \"adamw_torch_fused\",         # Optimizer type (fused AdamW for efficiency)\n",
    "        logging_steps = 1,                   # Log training metrics every N steps\n",
    "        per_device_train_batch_size = 1,     # Batch size per device (GPU/CPU) during training\n",
    "        gradient_accumulation_steps = 1,     # Number of steps to accumulate gradients before updating weights (increase for larger effective batch size)\n",
    "        num_generations = 4,                 # Number of generations per prompt (reduce if out of memory)\n",
    "        max_prompt_length = max_prompt_length,                   # Maximum length of the input prompt\n",
    "        max_completion_length = max_seq_length - max_prompt_length, # Maximum length of the generated completion\n",
    "        # num_train_epochs = 1,              # Number of training epochs (uncomment and set for full training run)\n",
    "        max_steps = 50,                      # Total number of training steps\n",
    "        save_steps = 50,                     # Save checkpoint every N steps\n",
    "        max_grad_norm = 0.1,                 # Maximum gradient norm for gradient clipping\n",
    "        report_to = \"none\",                  # Reporting backend (\"none\" disables reporting, can use \"wandb\" for Weights & Biases)\n",
    "        output_dir = \"outputs\"               # Directory to save model checkpoints and outputs\n",
    "        \n",
    "    )\n",
    "\n",
    "    trainer = GRPOTrainer(\n",
    "        model = model,\n",
    "        processing_class = tokenizer,\n",
    "        reward_funcs = [\n",
    "            match_format_exactly,\n",
    "            match_format_approx,\n",
    "            check_answer,\n",
    "            check_numbers,\n",
    "        ],\n",
    "        args = grpo_config,\n",
    "        train_dataset = dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained('outputs/gemma-3-tune1')\n",
    "    tokenizer.save_pretrained('outputs/gemma-3-tune1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ec8d7",
   "metadata": {},
   "source": [
    "# Evaluate pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26709f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.5: Fast Gemma3 patching. Transformers: 4.53.2. vLLM: 0.9.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.622 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# load the model weights\n",
    "# model.save_pretrained('outputs/gemma-3-tune1')\n",
    "model, tokenizer = FastModel.from_pretrained('outputs/gemma-3-tune1')\n",
    "print(type(model))\n",
    "print(type(tokenizer))\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4542336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,    105,   2364,    107,   3048,    659,   2238,    496,   2608,\n",
      "         236764,   1751,   1003,    506,   2608,    532,   2847,    822,  26149,\n",
      "         236761, 236743,    107,    140,  24698,    625,   1534,    655,   3041,\n",
      "         236779,  25421, 236779,    725, 236813,    532,    655,    643, 236779,\n",
      "          25421, 236779,    725,  24449,   4298,   2847,    822,   3465,    107,\n",
      "            140,  19195,    655, 213910,   2588, 213910, 236813,    108,   3689,\n",
      "            563,    506,   6281,   5989,    529, 236743, 236770, 236771, 236770,\n",
      "         236771, 236881,    106,    107,    105,   4368,    107]],\n",
      "       device='cuda:0')\n",
      "<class 'torch.Tensor'>\n",
      "<bos><start_of_turn>user\n",
      "You are given a problem, think about the problem and provide your workout. \n",
      "    Place it between <start_working_out> and <end_working_out>. Then provide your solution\n",
      "    between <SOLUTION></SOLUTION>\n",
      "\n",
      "What is the square root of 1010?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "<START_WORKING_OUT>\n",
      "1010 = 2 * 505\n",
      "âˆš1010 = âˆš(2 * 505) = âˆš(2 * 505) = 2 * âˆš505\n",
      "âˆš505 = âˆš(5 * 10\n"
     ]
    }
   ],
   "source": [
    "# Sample inference\n",
    "messages = [\n",
    "    {'role': 'system', \"content\": system_prompt},\n",
    "    {'role': 'user', \"content\": \"What is the square root of 1010?\"},\n",
    "]\n",
    "\n",
    "token_ids = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize=True\n",
    ").to('cuda')\n",
    "\n",
    "print(token_ids)\n",
    "print(type(token_ids))\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "\n",
    "output = model.generate(\n",
    "    token_ids,\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    # streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4dc06fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 1319\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "test_ds = load_dataset('openai/gsm8k', 'main', split=\"test\")\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dcd0ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    }
   ],
   "source": [
    "a = x if (x:=89) is not None else None\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "136f0db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 1319\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/1319 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate(model, question, **kwargs):\n",
    "    MAX_NEW_TOKENS = kwargs.get(\"max_new_tokens\", 64)\n",
    "\n",
    "    # Sample inference\n",
    "    messages = [\n",
    "        {'role': 'system', \"content\": system_prompt},\n",
    "        {'role': 'user', \"content\": question},\n",
    "    ]\n",
    "\n",
    "    token_ids = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt = True,\n",
    "        return_tensors = \"pt\",\n",
    "        tokenize=True\n",
    "    ).to('cuda')\n",
    "\n",
    "    # from transformers import TextStreamer\n",
    "\n",
    "    output = model.generate(\n",
    "        token_ids,\n",
    "        max_new_tokens = MAX_NEW_TOKENS, # Increase for longer outputs!\n",
    "        # Recommended Gemma-3 settings!\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    )\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_answer_from_completion(output: torch.Tensor):\n",
    "    assert type(output) == torch.Tensor, 'Wrong type: output type must be torch.tensor'\n",
    "\n",
    "    completion = tokenizer.decode(output[0])\n",
    "    return x if (x:=extract_response(completion)) is not None else None, completion\n",
    "\n",
    "\n",
    "print(test_ds)\n",
    "logging.info(\"##### EVALUATION #####\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "correct_answer = 0\n",
    "total = 0 \n",
    "for X in tqdm(test_ds, desc=\"Evaluating\"):\n",
    "    total += 1\n",
    "    try:\n",
    "        question = X[\"question\"]\n",
    "        answer = extract_hash_answer(X['answer'])\n",
    "        y = generate(model, question, max_new_tokens = 128)\n",
    "        pred_answer, completion = get_answer_from_completion(y)\n",
    "\n",
    "        # Log for later analysis\n",
    "\n",
    "        logger.info('question\\t' + question)\n",
    "        logger.info('target_answer\\t'+  answer)\n",
    "        logger.info('pred_answer\\t'+ pred_answer)\n",
    "        logger.info('completion\\t'+ completion)\n",
    "\n",
    "        correct_answer += 1 if (pred_answer != None and pred_answer == answer) else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error occured, check log\")\n",
    "        logger.error(\"Error in this question \", question)\n",
    "    \n",
    "    accuracy = (correct_answer / total) * 100\n",
    "    print(f'Accuracy = {accuracy:.2f}')\n",
    "    logger.info(f'Accuracy = {accuracy:.2f}')\n",
    "    logger.info(\"################################################\")\n",
    "    break\n",
    "\n",
    "# generate(model, test_ds[0]['question'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efa45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74b56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a5b908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# x = torch.tensor([1,2,3,4]).numpy()\n",
    "# y = torch.tensor([1,2,4,4]).numpy()\n",
    "\n",
    "# res = accuracy_score(x,y)\n",
    "# print(res)\n",
    "# print(type(res))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76d5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528e7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8fafa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
